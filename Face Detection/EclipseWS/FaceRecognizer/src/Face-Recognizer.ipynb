{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#abs_file_name = \"/Users/user170/Developments/Personal-Dev./Machine-Learning/Data/Face-SJC/employee-faces.pkl\"\n",
    "abs_file_name = \"/mnt/SharedData/Development/Personal_Dev/Machine-Learning/Data/Face/Face-SJC/employee-faces.pkl\"\n",
    "\n",
    "def unpickle(file):\n",
    "    with open(file, 'rb') as fo:\n",
    "        data = pickle.load(fo, encoding='bytes')\n",
    "    return data\n",
    "\n",
    "dataset = unpickle(abs_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
=======
>>>>>>> master
   "metadata": {},
   "outputs": [],
   "source": [
    "features = dataset['data']\n",
    "labels = dataset['labels']\n",
    "ref_labels = dataset['ref_labels']\n",
    "\n",
    "print(\"features shape: \", features.shape)\n",
    "print(\"labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing: Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scailing(img, new_min = 0, new_max = 1):\n",
    "    new_img = img.copy()\n",
    "    new_img = cv2.normalize(new_img, dst=None, alpha=new_min, beta=new_max, norm_type=cv2.NORM_MINMAX, dtype=cv2.CV_32F)\n",
    "\n",
    "    return new_img\n",
    "\n",
    "# # Normalization for object images\n",
    "def normalization(dataset):\n",
    "    new_features = []\n",
    "    \n",
    "    for f in tqdm(dataset):\n",
    "        #f_gray = self.bgr2gray(f)\n",
    "        #f_norm = scailing(f, new_min=0, new_max=1)\n",
    "        #print(\"feature1 < min: {0} | max: {1} >\".format(np.min(f_norm), np.max(f_norm)))\n",
    "        new_features.append(scailing(f, new_min=0, new_max=1))\n",
    "        \n",
    "    return np.array(new_features)\n",
    "    \n",
    "#labels = labels.reshape(-1, 1)\n",
    "features = normalization(features)\n",
    "#labels = labels.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Dataset preprocessing: Onthot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.one_hot(labels, labels.max()+1, axis=1)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    #sess.run(tf.global_variables_initializer())\n",
    "    labels = sess.run(tf.one_hot(labels, labels.max()+1, axis=1))\n",
    "\n",
    "print(\"Onehot labels shape: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset preprocessing: Train set and Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(\n",
    "    features, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "print(\"(train shape) features: {0}, labels: {1}\".format(features_train.shape, labels_train.shape))\n",
    "print(\"(test shape) features: {0}, labels: {1}\".format(features_test.shape, labels_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def pseudo_test_dataset():\n",
    "    #a = features_test[labels_test[labels_test == 0][0]]\n",
    "    #plt.imshow(a)\n",
    "\n",
    "    idx = 0\n",
    "    \n",
    "    '''\n",
    "    print(\"{0}: \".format(idx), labels_test[idx]) # 0\n",
    "    print(\"{0}: \".format(idx+1), labels_test[idx+1]) # 1\n",
    "    print(\"{0}: \".format(idx+3), labels_test[idx+3]) # 3\n",
    "    print(\"{0}: \".format(idx+4), labels_test[idx+4]) # 4\n",
    "    print(\"{0}: \".format(idx+6), labels_test[idx+6]) # 6\n",
    "    print(\"{0}: \".format(idx+7), labels_test[idx+7]) # 7\n",
    "    '''\n",
    "    \n",
    "    test_dataset =  np.expand_dims(features_test[0], axis=0)\n",
    "\n",
    "    test_dataset = np.vstack([test_dataset, np.expand_dims(features_test[1], axis=0)])\n",
    "    test_dataset = np.vstack([test_dataset, np.expand_dims(features_test[3], axis=0)])\n",
    "    test_dataset = np.vstack([test_dataset, np.expand_dims(features_test[4], axis=0)])\n",
    "    test_dataset = np.vstack([test_dataset, np.expand_dims(features_test[6], axis=0)])\n",
    "    test_dataset = np.vstack([test_dataset, np.expand_dims(features_test[7], axis=0)])\n",
    "\n",
    "    print(test_dataset.shape)\n",
    "\n",
    "    for i in range(5):\n",
    "        test_dataset = np.vstack([test_dataset, test_dataset])\n",
    "\n",
    "    print(test_dataset.shape)\n",
    "    \n",
    "    return test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resnet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class netParam(object):\n",
    "    def __init__(self, epoch = 5000, batch_size = 1000, learning_rate = 0.001):\n",
    "        # Input nodes\n",
    "        self.in_feature_shape = 224\n",
    "        self.in_feature_channels = 3\n",
    "        \n",
    "        self.out_size = 9\n",
    "        \n",
    "        self._input_nd = tf.placeholder(\n",
    "            tf.float32, shape=[None, self.in_feature_shape, self.in_feature_shape, self.in_feature_channels], name=\"input\")\n",
    "        self._labels_nd = tf.placeholder(tf.int32, shape=[None, self.out_size], name=\"labels\")\n",
    "        \n",
    "        '''\n",
    "        # Layer nodes\n",
    "        self._logits_nd = None\n",
    "        self._loss_nd = None\n",
    "        self._train_optimizer_nd = None\n",
    "        self._pred_nd = None\n",
    "        self._acc_nd = None\n",
    "        '''\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.kernel_1x1 = 1\n",
    "        self.kernel_2x2 = 2\n",
    "        self.kernel_3x3 = 3\n",
    "        \n",
    "        self.depth_64 = 64\n",
    "        self.depth_128 = 128\n",
    "        self.layer_3_depth = 48\n",
    "        self.layer_4_depth = 96\n",
    "        self.layer_5_depth = 192\n",
    "\n",
    "        self.dense_4096 = 4096\n",
    "        self.dense_1000 = 1000\n",
    "        self.dense_output = 2\n",
    "\n",
    "        self.stride_1x1 = 1\n",
    "        self.stride_2x2 = 2\n",
    "        \n",
    "        # Training parameters\n",
    "        self._epoch = epoch\n",
    "        self._batch_size = batch_size\n",
    "        self._rl = learning_rate\n",
    "        \n",
    "    def _filter_var(cls, kernel, in_depth, out_depth, node_name):\n",
    "        return tf.Variable(\n",
    "            tf.truncated_normal((kernel, kernel, in_depth, out_depth), stddev=0.1, dtype=tf.float32),\n",
    "            name=node_name+\"_filter\")\n",
    "    \n",
    "    def _bias_var(cls, size, node_name):\n",
    "        return tf.Variable(\n",
    "            tf.zeros([size], dtype=tf.float32), \n",
    "            name=node_name+\"_bias\")\n",
    "\n",
    "    def _dense_var(cls, in_size, out_size, node_name):\n",
    "        return tf.Variable(\n",
    "            tf.truncated_normal([in_size, out_size], stddev=0.1, dtype=tf.float32),\n",
    "            name=node_name+\"_dense\")\n",
    "    \n",
    "    def _max_pool(cls, in_node, kernel_size, stride_size, pad_type, node_name):\n",
    "        return tf.nn.max_pool(in_node, \n",
    "                              ksize=[1, kernel_size, kernel_size, 1], \n",
    "                              strides=[1, stride_size, stride_size, 1], \n",
    "                              padding=pad_type,\n",
    "                              name=node_name+\"/Max_pool\")\n",
    "    \n",
    "    def _avg_pool(cls, in_node, kernel_size, stride_size, pad_type, node_name):\n",
    "        return tf.nn.avg_pool(in_node, \n",
    "                              ksize=[1, kernel_size, kernel_size, 1], \n",
    "                              strides=[1, stride_size, stride_size, 1], \n",
    "                              padding=pad_type,\n",
    "                              name=node_name+\"/Avg_pool\")\n",
    "    \n",
    "    def _convolutional_layer(self, in_node, kernel_size, out_depth, stride, padding, name, use_bias=False):\n",
    "        with tf.variable_scope(name):\n",
    "            # Weight to be learned\n",
    "            conv_filter = self._filter_var(\n",
    "                kernel = kernel_size,\n",
    "                in_depth=in_node.get_shape().as_list()[-1],\n",
    "                out_depth=out_depth,\n",
    "                node_name=name)\n",
    "            \n",
    "            # Convolutional node\n",
    "            conv = tf.nn.conv2d(in_node, conv_filter, [1, stride, stride, 1], padding=padding)\n",
    "            \n",
    "            # Biase to be learned\n",
    "            if use_bias:\n",
    "                conv_bias = self._bias_var(size=out_depth, node_name=name)\n",
    "                conv = tf.nn.bias_add(conv, conv_bias)\n",
    "            \n",
    "            #conv = tf.nn.relu(conv)\n",
    "\n",
    "            return conv\n",
    "\n",
    "    def _dense_layer(self, in_node, dense_size, relu, name):\n",
    "        #print(in_node.get_shape().as_list())\n",
    "        dense_weight = self._dense_var(in_size=in_node.get_shape().as_list()[-1], out_size=dense_size, node_name=name)\n",
    "        dense_bias = self._bias_var(size=dense_size, node_name=name)\n",
    "\n",
    "        if relu == \"relu\":\n",
    "            return tf.nn.relu(tf.add(tf.matmul(in_node, dense_weight), dense_bias))\n",
    "        else:\n",
    "            return tf.add(tf.matmul(in_node, dense_weight), dense_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet(netParam):\n",
    "    def __init__(self, epoch = 5000, batch_size = 1000, learning_rate = 0.001):\n",
    "        super(Resnet, self).__init__(epoch, batch_size, learning_rate)\n",
    "\n",
    "    def identity_function(self, input_node, filter_depth, identity_name, half=False):\n",
    "        with tf.variable_scope(identity_name):\n",
    "            if half:\n",
    "                input_node = self._convolutional_layer(input_node, self.kernel_3x3, filter_depth, self.stride_2x2, \"SAME\", \"conv_in\", False)\n",
    "                #print(input_node)\n",
    "            conv1 = self._convolutional_layer(input_node, self.kernel_3x3, filter_depth, self.stride_1x1, \"SAME\", \"conv1\", False)\n",
    "            #print(conv1)\n",
    "            relu = tf.nn.relu(conv1)\n",
    "            #print(relu)\n",
    "            conv2 = self._convolutional_layer(relu, self.kernel_3x3, filter_depth, self.stride_1x1, \"SAME\", \"conv2\", False)\n",
    "            #print(conv2)\n",
    "            \n",
    "            #conv2 = tf.multiply(conv2, 0.1)\n",
    "            conv2 = tf.multiply(conv2, 0.01)\n",
    "            skip = tf.add(input_node, conv2, name=\"skip\")\n",
    "            #skip = tf.multiply(input_node, conv2, name=\"skip\")\n",
    "            relu = tf.nn.relu(skip)\n",
    "            \n",
    "            return relu\n",
    "    def simple_CNN(self, input_node):\n",
    "        # CONVOLUTIONAL 1\n",
    "        conv1 = self._convolutional_layer(input_node, 3, 32, 1, \"VALID\", \"conv1\")\n",
    "        \n",
    "        # FLATTEN\n",
    "        relu1_shape = conv1.get_shape().as_list()\n",
    "        conv1_flat = tf.reshape(conv1, [-1, relu1_shape[1]*relu1_shape[2]*relu1_shape[3]])\n",
    "        \n",
    "        # DENSE 1\n",
    "        dense1 = self._dense_layer(conv1_flat, 128, \"relu\", \"dense1\")\n",
    "\n",
    "        # DENSE 2\n",
    "        dense2 = self._dense_layer(dense1, 10, \"None\", \"dense2\")\n",
    "        \n",
    "        return dense2\n",
    "        \n",
    "    def resnet(self, input_node):\n",
    "        print(\"|============================== MODEL INFO. ====================================|\")\n",
    "        print(\"  Input  : \", input_node)\n",
    "        # Convolutional layer 1: conv-> relu -> max_pool\n",
    "        #with tf.variable_scope(\"layer_1\"):\n",
    "        layer_1 = self._convolutional_layer(input_node, self.kernel_3x3, self.depth_64, self.stride_1x1, \"SAME\", \"layer_1\", False)\n",
    "        #print(\"  Layer 1: \", layer_1)\n",
    "        layer_1 = tf.nn.relu(layer_1, name=\"layer_1/Relu\")\n",
    "        #print(\"  Layer 1: \", layer_1)\n",
    "        layer_1 = self._max_pool(layer_1, self.kernel_2x2, self.stride_2x2, \"VALID\", \"layer_1\")\n",
    "        print(\"  Layer 1: \", layer_1)\n",
    "        \n",
    "        layer_2 = self.identity_function(layer_1, 64, \"layer_2\")\n",
    "        print(\"  Layer 2: \", layer_2)\n",
    "        \n",
    "        layer_3 = self.identity_function(layer_2, 64, \"layer_3\")\n",
    "        print(\"  Layer 3: \", layer_3)\n",
    "        \n",
    "        layer_4 = self.identity_function(layer_3, 64, \"layer_4\")\n",
    "        print(\"  Layer 4: \", layer_4)\n",
    "        \n",
    "        layer_5 = self.identity_function(layer_4, self.depth_128, \"layer_5\", True)\n",
    "        print(\"  Layer 5: \", layer_5)\n",
    "        \n",
    "        layer_6 = self.identity_function(layer_5, self.depth_128, \"layer_6\")\n",
    "        print(\"  Layer 6: \", layer_6)\n",
    "        \n",
    "        layer_7 = self.identity_function(layer_5, self.depth_128, \"layer_7\")\n",
    "        print(\"  Layer 7: \", layer_7)\n",
    "        \n",
    "        layer_8 = self.identity_function(layer_5, self.depth_128, \"layer_8\")\n",
    "        print(\"  Layer 8: \", layer_8)\n",
    "        \n",
    "        layer_8_avg_pool = self._avg_pool(layer_8, self.kernel_2x2, self.stride_2x2, \"VALID\", \"layer_8\")\n",
    "        print(\"  Layer 8: \", layer_8_avg_pool)\n",
    "        \n",
    "        layer_8_pool_shape = layer_8_avg_pool.get_shape().as_list()\n",
    "        layer_8_flat = tf.reshape(layer_8_avg_pool, [-1, layer_8_pool_shape[1]*layer_8_pool_shape[2]*layer_8_pool_shape[3]])\n",
    "        print(\"  Layer 8: \", layer_8_flat)\n",
    "        \n",
    "        #layer_8_flat_shape = layer_8_flat.get_shape().as_list()\n",
    "        logits = self._dense_layer(layer_8_flat, self.out_size, \"None\", name=\"logits\")\n",
    "        print(\"  Logits : \", logits)\n",
    "        print(\"|===============================================================================|\")\n",
    "        \n",
    "        return logits\n",
    "\n",
    "    def _cost_function(self, logits):\n",
    "        # CONST FUNCTION\n",
    "        #loss_nd = tf.losses.sparse_softmax_cross_entropy(labels=self._labels_nd, logits=logits)\n",
    "        print(\"KKK: \", logits)\n",
    "        loss_nd = tf.nn.softmax_cross_entropy_with_logits(labels=self._labels_nd, logits=logits)\n",
    "        return loss_nd\n",
    "\n",
    "    def _optimizer(self, loss):\n",
    "        # OPTIMIZER\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self._rl)\n",
    "        train_op = optimizer.minimize(loss, global_step=tf.train.get_global_step())\n",
    "        return train_op\n",
    "\n",
    "    def _prediction(self, logits):\n",
    "        # PREDICTION\n",
    "        pred = tf.argmax(logits, 1)\n",
    "        return pred\n",
    "\n",
    "    def _accuracy(self, pred):\n",
    "        # ACCURACY\n",
    "        acc = tf.equal(pred, tf.argmax(self._labels_nd, 1))\n",
    "        return acc\n",
    "        \n",
    "    def _accuracy_info(self, iterator, session, accuracy_node, batch_x, batch_y, test_x, test_y):\n",
    "        train_acc = session.run(accuracy_node, feed_dict={self._input_nd: batch_x, self._labels_nd: batch_y})\n",
    "        \n",
    "        batch_index = np.random.choice(len(test_x), size=self._batch_size)\n",
    "        test_batch_x = test_x[batch_index]\n",
    "        test_batch_y = test_y[batch_index]\n",
    "        \n",
    "        eval_acc = session.run(accuracy_node, feed_dict={self._input_nd: test_batch_x, self._labels_nd: test_batch_y})\n",
    "\n",
    "        #print(\"({0}/{1}) TRAIN ACC.: {2} %\"\\\n",
    "        #      .format(iter, self._epoch, np.sum(train_acc)/len(train_acc)*100))\n",
    "\n",
    "        print(\"({0}/{1}) TRAIN ACC.: {2} % | EVAL ACC.: {3} %\"\\\n",
    "              .format(iterator, self._epoch, np.sum(train_acc)/len(train_acc)*100, np.sum(eval_acc)/len(eval_acc)*100))\n",
    "        \n",
    "    def train(self):\n",
    "        logits = self.resnet(self._input_nd)\n",
    "        #logits = self.simple_CNN(self._input_nd)\n",
    "        loss = self._cost_function(logits)\n",
    "        optimizer = self._optimizer(loss)\n",
    "        pred = self._prediction(logits)\n",
    "        acc = self._accuracy(pred)\n",
    "        \n",
    "        #fig, ax = plt.subplots(1, 4, figsize=(10, 5))\n",
    "\n",
    "        with tf.Session() as sess:\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "\n",
    "            for iterator in range(self._epoch):\n",
    "                batch_index = np.random.choice(len(features_train), size=self._batch_size)\n",
    "                #self.batch_x = features[batch_index]\n",
    "                #self.batch_y = labels[batch_index]\n",
    "\n",
    "                batch_x = features_train[batch_index]\n",
    "                batch_y = labels_train[batch_index]\n",
    "\n",
    "                '''\n",
    "                a = sess.run(logits, feed_dict={self._input_nd: batch_x, self._labels_nd: batch_y})\n",
    "                print(np.shape(a))\n",
    "                '''\n",
    "                sess.run(optimizer, feed_dict={self._input_nd: batch_x, self._labels_nd: batch_y})\n",
    "\n",
    "                if iterator % 10 == 0:\n",
    "                    self._accuracy_info(iterator, sess, acc, batch_x, batch_y, features_test, labels_test)\n",
    "                    \n",
    "                    pred_labels = sess.run(pred, feed_dict={self._input_nd: features_test[:self._batch_size]})\n",
    "                    print(\"\\t\", np.argmax(labels_test[:self._batch_size][:10], axis=1))\n",
    "                    print(\"\\t\", pred_labels[:10], \"\\n\")\n",
    "                    #print(np.sum(pred_labels != 0))\n",
    "                \n",
    "                '''\n",
    "                for i in range(4):\n",
    "                    ax[i].imshow(features_test[:self._batch_size][i])\n",
    "                    ax[i].set_title(pred_labels[i])\n",
    "\n",
    "                plt.pause(0.001)\n",
    "                plt.show()\n",
    "                '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rn = Resnet(epoch = 1000, batch_size = 100, learning_rate = 0.001)\n",
    "rn.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
