struct TensorMatConverter {
    cv::Mat tensor2CVMat(at::Tensor &&imgTensor, int outputSize) {
        // Convert the image Tensor to cv::Mat with CV_8UC3 data type
        int cvMatSize[2] = {(int)imgTensor.size(1), (int)imgTensor.size(2)};
        cv::Mat imgCVB(2, cvMatSize, CV_8UC1, imgTensor[0].data_ptr());
        cv::Mat imgCVG(2, cvMatSize, CV_8UC1, imgTensor[1].data_ptr());
        cv::Mat imgCVR(2, cvMatSize, CV_8UC1, imgTensor[2].data_ptr());

        // Merge each channel to create colour cv::Mat
        cv::Mat imgCV; // Merged output cv::Mat
        std::vector<cv::Mat> channels;
        channels.push_back(imgCVB);
        channels.push_back(imgCVG);
        channels.push_back(imgCVR);
        cv::merge(channels, imgCV);

        // Resize cv::Mat
        int newH = outputSize;
        int newW = outputSize;

        cv::Mat resizedImage;

        cv::resize(imgCV, resizedImage, cv::Size2d(newH, newW), 0, 0, cv::INTER_LINEAR);

        return resizedImage;
    }

    at::Tensor cvMat2Tensor(cv::Mat imgCVMat) {
        // Convert the image and label to a tensor.
        torch::TensorOptions imgOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
        torch::Tensor imgTensor = torch::from_blob(imgCVMat.data, {imgCVMat.rows, imgCVMat.cols, 3}, imgOptions);
        imgTensor = imgTensor.permute({2, 0, 1}); // convert to CxHxW

        return imgTensor;
    }
};

template <typename T = torch::data::Example<>>
struct MyResize;

/// Custom Stack
template <>
struct MyResize<torch::data::Example<>> : public torch::data::transforms::Collation<torch::data::Example<>> {
    private:
        int _outputSize;

    public:
        MyResize(int outputSize): _outputSize(outputSize){std::cout << "MyResize Constructor\n";}; // Constructor

        torch::data::Example<> apply_batch(std::vector<torch::data::Example<>> examples) override {
            if (typeid(_outputSize) != typeid(int)){
                std::fprintf(stderr, "Rescale ERROR: data type of output size is incorrect.\n");
                exit(-1);
            }
            
            std::vector<torch::Tensor> data, targets;
            data.reserve(examples.size());
            targets.reserve(examples.size());

            at::Tensor outputData;
            at::Tensor outputTargets;

            std::cout << "MyResize TEST 1\n";

            TensorMatConverter tmc = TensorMatConverter();
            std::cout << "MyResize TEST 2\n";

            for (auto& example : examples) {
                cv::Mat imgData = tmc.tensor2CVMat(std::move(example.data), _outputSize);
                std::cout << "MyResize TEST 3\n";
                
                data.push_back(std::move(tmc.cvMat2Tensor(imgData)));
                outputData = tmc.cvMat2Tensor(imgData);
                std::cout << "MyResize TEST 4\n";

                targets.push_back(std::move(example.target));
                outputTargets = example.target;
                std::cout << "MyResize TEST 5\n";
            }

            //return {torch::stack(data), torch::stack(targets)};
            return {outputData, outputTargets};
        }
};

=================================================================================================================================================


class Rescale {
    private:
        int _outputSizeInt;
        std::tuple<int, int> _outputSizeTuple;
        std::tuple<cv::Mat, std::vector<float>> _resizedDataCVandFloat;

        int _newW, _newH;

        const std::type_info &_typeId;
    
    public:
        // GETTERs
        std::tuple<cv::Mat, std::vector<float>> getResizedDataCVandFloat(){return _resizedDataCVandFloat;};

        // METHODs
        Rescale(int outputSize): _outputSizeInt(outputSize), _typeId(typeid(int)) {
            if (typeid(outputSize) != typeid(int)){
                std::fprintf(stderr, "Rescale ERROR: data type of output size is incorrect.\n");
                exit(-1);
            }

            //std::fprintf(stdout, "outputSizeInt: %d\n", _outputSizeInt);
        };

        Rescale(std::tuple<int, int> outputSize): _outputSizeTuple(outputSize), _typeId(typeid(std::tuple<int, int>)) {
            if (typeid(outputSize) != typeid(std::tuple<int, int>)) {
                std::fprintf(stderr, "Rescale ERROR: data type of output size is incorrect.\n");
                exit(-1);
            }

            auto [A, B] = _outputSizeTuple;
            //std::fprintf(stdout, "outputSizeTuple: %d, %d\n", A, B);
        };

        Rescale operator() (cv::Mat const& image, std::vector<int> const &landmarks) {
            int w = image.rows;
            int h = image.cols;
            
            //std::cout << image.size << std::endl;
            //std::fprintf(stdout, "W: %d, H: %d\n", w, h);

            if (_typeId == typeid(int) and _outputSizeInt != 0) {
                if (h > w) {
                    //std::cout << "W<H" << std::endl;
                    //_newH =  (int)_outputSizeInt * h/(float)w;
                    _newH =  _outputSizeInt * h/w;
                    _newW = _outputSizeInt;
                }
                else {
                    //std::cout << "W>H" << std::endl;
                    _newH = _outputSizeInt;
                    //_newW =  (int)_outputSizeInt * w/(float)h;
                    _newW =  _outputSizeInt * w/h;
                }
            }
            else {
                _newH = h;
                _newW = w;
            }
            
            if(std::get<0>(_outputSizeTuple) != 0 and std::get<1>(_outputSizeTuple) != 0){
                auto [newW, newH] = _outputSizeTuple;
                _newW = newW;
                _newH = newH;
            }
            else {
                _newW = w;
                _newH = h;
            }
            
            cv::Mat resizedImage;
            std::vector<float> resizedLabel;

            // Rescale the image
            cv::resize(image, resizedImage, cv::Size2d(_newH, _newW), 0, 0, cv::INTER_LINEAR);

            // Rescale the labels
            int coord = 0;
            for (auto landmark: landmarks) {
                if (++coord % 2 == 1) resizedLabel.push_back(landmark * _newH/h);
                else resizedLabel.push_back(landmark * _newW/w);
            }
            //std::fprintf(stdout, "newW: %d, newH: %d\n", _newW, _newH);

            //std::cout << resizedImage.size << std::endl;

            //for (auto landmark: resizedLabel) {
                //std::cout << landmark << ", ";
            //}
            //std::cout << std::endl;

            _resizedDataCVandFloat = std::make_tuple(resizedImage, resizedLabel); // data to be returned

            return *this;
        }


        Rescale operator() (torch::Tensor const &imgTensor, torch::Tensor const &labelTensor) {
            // Convert the image Tensor to cv::Mat with CV_8UC3 data type
            int cvMatSize[2] = {(int)imgTensor.size(1), (int)imgTensor.size(2)};
            cv::Mat imgCVB(2, cvMatSize, CV_8UC1, imgTensor[0].data_ptr());
            cv::Mat imgCVG(2, cvMatSize, CV_8UC1, imgTensor[1].data_ptr());
            cv::Mat imgCVR(2, cvMatSize, CV_8UC1, imgTensor[2].data_ptr());

            // Merge each channel to create colour cv::Mat
            cv::Mat imgCV; // Merged output cv::Mat
            std::vector<cv::Mat> channels;
            channels.push_back(imgCVB);
            channels.push_back(imgCVG);
            channels.push_back(imgCVR);
            cv::merge(channels, imgCV);

            int w = imgCV.rows;
            int h = imgCV.cols;

            if (_typeId == typeid(int)) {
                if (h > w) {
                    //std::cout << "W<H" << std::endl;
                    //_newH =  (int)_outputSizeInt * h/(float)w;
                    _newH =  _outputSizeInt * h/w;
                    _newW = _outputSizeInt;
                }
                else {
                    //std::cout << "W>H" << std::endl;
                    _newH = _outputSizeInt;
                    //_newW =  (int)_outputSizeInt * w/(float)h;
                    _newW =  _outputSizeInt * w/h;
                }
            }
            else {
                auto [newW, newH] = _outputSizeTuple;
                _newW = newW;
                _newH = newH;
            }

            cv::Mat resizedImage;
            std::vector<float> resizedLabel;

            // Rescale the image
            cv::resize(imgCV, resizedImage, cv::Size2d(_newH, _newW), 0, 0, cv::INTER_LINEAR);

            // Convert the label Tensor to vector
            std::vector<float> resizedLabelVector;

            for (int i=0; i<labelTensor.size(1); ++i) {
                resizedLabelVector.push_back(labelTensor[0][i].item<float>());
            }

            // Rescale the labels
            int coord = 0;
            for (auto landmark: resizedLabelVector) {
                if (++coord % 2 == 1) resizedLabel.push_back(landmark * _newH/h);
                else resizedLabel.push_back(landmark * _newW/w);
            }

            _resizedDataCVandFloat = std::make_tuple(resizedImage, resizedLabel); // data to be returned
            //_resizedDataTensor = {};
            return *this;
        }
};


=================================================================================================================================================

torch::data::Example<> CustomDataset::get(size_t index)
{
    //if (_verbose) std::fprintf(stdout, "CustomDataset get called\n");

    auto [imgName, label] = _dataset[index];

    std::string imgPath = _locImages;
    imgPath += imgName;
    
    //std::cout << imgPath << std::endl;

    // Load image with OpenCV.
    cv::Mat img = cv::imread(imgPath);
    //if (_verbose) checkcvMatNan(img, "CV_8UC3");

    img.convertTo(img, CV_32FC3); // Convert CV_8UC3 data type to CV_32FC3
    //if (_verbose) checkcvMatNan(img, "CV_32FC3");

    // Rescale
    /*auto rescale = Rescale(_rescale);
    rescale(img, label);
    auto [rImg, rLabel] = rescale.getResizedDataCVandFloat();
    //img = rImg/255; // rescale to [0, 1]
    img = rImg;
    */
    //auto rLabel = label;
    //std::cout << rLabel << std::endl;

    // Convert the image and label to a tensor.
    torch::TensorOptions imgOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
    torch::Tensor imgTensor = torch::from_blob(img.data, {img.rows, img.cols, 3}, imgOptions);
    imgTensor = imgTensor.permute({2, 0, 1}); // convert to CxHxW
    //if (_verbose) printf("nan in the img Tensor: %s\n", torch::isnan(imgTensor).sum().item<int>() ? "nan detected" : "nan not detected");
    //std::cout << imgTensor.sizes() << std::endl;

    // Convert int label to a tensor
    float labelsArr[label.size()];
    std::copy(label.begin(), label.end(), labelsArr);

    torch::TensorOptions labelOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
    //torch::TensorOptions labelOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
    torch::Tensor labelTensor = torch::from_blob(labelsArr, {1, (signed long) label.size()}, labelOptions);
    //labelTensor = labelTensor.div(std::get<0>(_rescale));
    //if (_verbose) printf("nan in the label Tensor: %s\n", torch::isnan(labelTensor).sum().item<int>() ? "nan detected" : "nan not detected");

    //checkTensorImgAndLandmarksV2(img, label, imgTensor, labelTensor);
    //checkTensorImgAndLandmarksV1(std::move(imgTensor.clone()), std::move(labelTensor.clone()));

    return {imgTensor.clone(), labelTensor.clone()};
}

=================================================================================================================================================
HOW TO TRANSFORM ONLY INPUT DATA OF EXAMPLE DATA TYPE V1

template <typename Target = torch::Tensor>
struct MyResize : public torch::data::transforms::TensorTransform<Target> {
  /// Constructs a `Normalize` transform. The mean and standard deviation can be
  /// anything that is broadcastable over the input tensors (like single
  /// scalars).
  MyResize(int newWH)
      : _newWH(newWH) {}

  torch::Tensor operator()(torch::Tensor imgTensor) {
    // Convert the image Tensor to cv::Mat with CV_8UC3 data type
    int cvMatSize[2] = {(int)imgTensor.size(1), (int)imgTensor.size(2)};
    cv::Mat imgCVB(2, cvMatSize, CV_8UC1, imgTensor[0].data_ptr());
    cv::Mat imgCVG(2, cvMatSize, CV_8UC1, imgTensor[1].data_ptr());
    cv::Mat imgCVR(2, cvMatSize, CV_8UC1, imgTensor[2].data_ptr());

    // Merge each channel to create colour cv::Mat
    cv::Mat imgCV; // Merged output cv::Mat
    std::vector<cv::Mat> channels;
    channels.push_back(imgCVB);
    channels.push_back(imgCVG);
    channels.push_back(imgCVR);
    cv::merge(channels, imgCV);

    cv::Mat resizedImage;

    cv::resize(imgCV, resizedImage, cv::Size2d(_newWH, _newWH), 0, 0, cv::INTER_LINEAR);

    // Convert the image and label to a tensor.
    torch::TensorOptions imgOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
    torch::Tensor returnTensor = torch::from_blob(resizedImage.data, {resizedImage.rows, resizedImage.cols, 3}, imgOptions);
    returnTensor = returnTensor.permute({2, 0, 1}); // convert to CxHxW

    return returnTensor;
  }

  int _newWH;
};


=================================================================================================================================================
HOW TO TRANSFORM ONLY INPUT DATA OF EXAMPLE DATA TYPE V2

template <typename Target>
struct ImgDataResize : public torch::data::transforms::TensorTransform<Target> {
    private:
        int _newWH;
    
    public:
        ImgDataResize(int newWH)
            : _newWH(newWH) {}

        torch::Tensor operator()(torch::Tensor inputTensor) {
            torch::Tensor imgTensor = inputTensor.toType(torch::kUInt8).clone();
            //std::cout << imgTensor << std::endl;
            //std::cout << imgTensor.max() << ", " << imgTensor.min() << std::endl;

            // Convert the image Tensor to cv::Mat with CV_8UC3 data type
            int cvMatSize[2] = {(int)imgTensor.size(1), (int)imgTensor.size(2)};
            cv::Mat imgCVB(2, cvMatSize, CV_8UC1, imgTensor[0].data_ptr());
            cv::Mat imgCVG(2, cvMatSize, CV_8UC1, imgTensor[1].data_ptr());
            cv::Mat imgCVR(2, cvMatSize, CV_8UC1, imgTensor[2].data_ptr());

            // Merge each channel to create colour cv::Mat
            cv::Mat imgCV; // Merged output cv::Mat
            std::vector<cv::Mat> channels;
            channels.push_back(imgCVB);
            channels.push_back(imgCVG);
            channels.push_back(imgCVR);
            cv::merge(channels, imgCV);

            cv::Mat resizedImage;
            cv::resize(imgCV, resizedImage, cv::Size2d(_newWH, _newWH), 0, 0, cv::INTER_LINEAR);
            //std::cout << resizedImage << std::endl;
            //std::cout << "IMG Sum: " << cv::sum(cv::sum(resizedImage)) << std::endl;
            // Convert the image.
            resizedImage.convertTo(resizedImage, CV_32FC3);
            torch::TensorOptions imgOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
            torch::Tensor returnTensor = torch::from_blob(resizedImage.data, {resizedImage.rows, resizedImage.cols, 3}, imgOptions);
            returnTensor = returnTensor.permute({2, 0, 1}); // convert to CxHxW
            
            std::cout << "Return Tensor: " << returnTensor.sum() << std::endl;

            return returnTensor.clone();
        }
};

=================================================================================================================================================
HOW TO USE PYTORCH LAMBDA

torch::data::Example<> test(torch::data::Example<> exampleInput) {
    auto imgTensor = exampleInput.data;
    auto labelTensor = exampleInput.target;

    // Convert the image Tensor to cv::Mat with CV_8UC3 data type
    int cvMatSize[2] = {(int)imgTensor.size(1), (int)imgTensor.size(2)};
    cv::Mat imgCVB(2, cvMatSize, CV_8UC1, imgTensor[0].data_ptr());
    cv::Mat imgCVG(2, cvMatSize, CV_8UC1, imgTensor[1].data_ptr());
    cv::Mat imgCVR(2, cvMatSize, CV_8UC1, imgTensor[2].data_ptr());

    // Merge each channel to create colour cv::Mat
    cv::Mat imgCV; // Merged output cv::Mat
    std::vector<cv::Mat> channels;
    channels.push_back(imgCVB);
    channels.push_back(imgCVG);
    channels.push_back(imgCVR);
    cv::merge(channels, imgCV);

    // Resize cv::Mat
    int newH = 200;
    int newW = 200;

    cv::Mat resizedImage;

    cv::resize(imgCV, resizedImage, cv::Size2d(newH, newW), 0, 0, cv::INTER_LINEAR);

    // Convert the image and label to a tensor.
    torch::TensorOptions imgOptions = torch::TensorOptions().dtype(torch::kFloat32).requires_grad(false);
    torch::Tensor returnTensor = torch::from_blob(resizedImage.data, {resizedImage.rows, resizedImage.cols, 3}, imgOptions);
    returnTensor = returnTensor.permute({2, 0, 1}); // convert to CxHxW

    return {returnTensor, labelTensor};
}

----->
        auto cds = CustomDataset(
            //"/DATASETs/Face/Landmarks/300W-Dataset/300W/face_landmarks.csv",
            //"/DATASETs/Face/Landmarks/300W-Dataset/300W/Data/",
            "/DATASETs/Face/Landmarks/Pytorch-Tutorial-Landmarks-Dataset/face_landmarks.csv",
            "/DATASETs/Face/Landmarks/Pytorch-Tutorial-Landmarks-Dataset/faces/",
            false)
            .map(torch::data::transforms::Lambda<torch::data::Example<>>(test))
            .map(torch::data::transforms::Stack<>());

