{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from PIL import Image\n",
    "from skimage import io, transform\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert matlab data to jpg image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = \"/home/user170/shared-data/Personal_Dev/Machine-Learning/Data/Depth/NYU-Depth-Dataset-V2/\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(data_folder+'nyu_depth_v2_labeled.mat', 'r') as mat_data:\n",
    "    data_len = len(mat_data['images'])\n",
    "    \n",
    "    for idx in range(data_len):\n",
    "        rgb_img = Image.fromarray(np.moveaxis(mat_data['images'][idx], [0, 1], [-1, -2]))\n",
    "        depth_img = Image.fromarray(np.moveaxis(mat_data['depths'][idx], [0], [-1]), mode='F')\n",
    "        \n",
    "        rgb_img.save(data_folder+\"Images/RGB/{0:04d}.jpg\".format(idx))\n",
    "        depth_img.save(data_folder+\"Images/Depth-TIFF/{0:04d}.tiff\".format(idx), \"TIFF\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Convert float to png using opencv\n",
    "import cv2\n",
    "\n",
    "with h5py.File(data_folder+'nyu_depth_v2_labeled.mat', 'r') as mat_data:\n",
    "    data_len = len(mat_data['images'])\n",
    "    \n",
    "    for idx in range(data_len):\n",
    "        #rgb_img = Image.fromarray(np.moveaxis(mat_data['images'][idx], [0, 1], [-1, -2]))\n",
    "        depth_img = np.moveaxis(mat_data['depths'][idx], [0], [-1])\n",
    "        cv2.imwrite(data_folder+\"Images/Depth/PNG/{0}.png\".format(idx), depth_img*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Images"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# PIL IMAGE USE\n",
    "rgb_img = Image.open(data_folder+\"Images/RGB/0000.jpg\")\n",
    "d_img = Image.open(data_folder+\"Images/Depth-TIFF/0000.tiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCIKIT IMAGE USE\n",
    "rgb_img = io.imread(os.path.join(data_folder, \"Images/RGB/0000.jpg\"))\n",
    "d_img = io.imread(os.path.join(data_folder, \"Images/Depth-TIFF/0000.tiff\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rgb_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(d_img)\n",
    "print(np.max(d_img))\n",
    "print(np.min(d_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadData(Dataset):\n",
    "    def __init__(self, root_dir, testset_ratio=0.3, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): Directory with RGB and Depth-TIFF images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        #self.mat_data = h5py.File(root_dir+mat_file, 'r')\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.testset_ratio = testset_ratio\n",
    "        \n",
    "        self.trainset_idx, self.testset_idx, self.trainset_num, self.testset_num = self._seperate_dataset()\n",
    "\n",
    "    def _seperate_dataset(self):\n",
    "        \"\"\"\n",
    "        Seperate Train and Test dataset\n",
    "        \n",
    "        Returns: random indice of train test datasets, and length of those.\n",
    "        \"\"\"\n",
    "        dataset_len = len(os.listdir(self.root_dir+\"Images/RGB/\"))\n",
    "        rnd_idx = [i for i in range(dataset_len)]\n",
    "        np.random.shuffle(rnd_idx)\n",
    "        split_thresh = int(dataset_len*(1-self.testset_ratio))\n",
    "\n",
    "        train_idx = rnd_idx[:split_thresh]\n",
    "        test_idx = rnd_idx[split_thresh:]\n",
    "\n",
    "        trainset_len = len(train_idx)\n",
    "        testset_len = len(test_idx)\n",
    "\n",
    "        return train_idx, test_idx, trainset_len, testset_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        self.dataset_len = len(os.listdir(self.root_dir+\"Images/RGB/\"))\n",
    "        return self.dataset_len\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        #rgb_img = Image.open(self.root_dir+'Images/RGB/{0:04d}.jpg'.format(self.trainset_idx[idx]))\n",
    "        #depth_img = Image.open(self.root_dir+'Images/Depth-TIFF/{0:04d}.tiff'.format(self.trainset_idx[idx]))\n",
    "        rgb_img = io.imread(self.root_dir+'Images/RGB/{0:04d}.jpg'.format(self.trainset_idx[idx]))\n",
    "        depth_img = io.imread(self.root_dir+'Images/Depth-TIFF/{0:04d}.tiff'.format(self.trainset_idx[idx]))\n",
    "            \n",
    "        if self.transform:\n",
    "            rgb_img = self.transform(rgb_img)\n",
    "            depth_img = self.transform(depth_img)\n",
    "        \n",
    "        #print(np.max(rgb_img.numpy()), np.min(rgb_img.numpy()))\n",
    "        #print(np.max(depth_img.numpy()), np.min(depth_img.numpy()))\n",
    "        sample = {'RGB':rgb_img, 'DEPTH':depth_img}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = LoadData(data_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataloader)):\n",
    "    sample = dataloader[i]\n",
    "    print(i, \n",
    "          '[RGB u: {:.2f}, std: {:.2f}]'.format(np.mean(sample['RGB']), np.std(sample['RGB'])),\n",
    "          '\\t[DEPTH u: {:.2f}, std: {:.2f}]'.format(np.mean(sample['DEPTH']), np.std(sample['DEPTH'])))\n",
    "    \n",
    "    if i == 3:\n",
    "        #plt.show()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarray in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample_img):\n",
    "        rgb_img, depth_img = sample['RGB'], sample['DEPTH']\n",
    "        \n",
    "        # swap colour axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        rgb_img = rgb_img.transpose((2, 0, 1))\n",
    "        #rgb_img = np.moveaxis(np.asarray(rgb_img), [-1], [0])\n",
    "        depth_img = depth_img\n",
    "        \n",
    "        return {'RGB': torch.from_numpy(rgb_img), 'DEPTH': torch.from_numpy(depth_img)}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# DIRECTLY SHOW A TENSOR IMAGES\n",
    "to_tensor = ToTensor()\n",
    "plt.imshow(to_tensor(dataloader[0])['RGB'].numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SHOW A NUMPY IMAGES CONVERTED FROM A TENSOR USING PERSONAL CLASS\n",
    "composed = transforms.Compose([ToTensor()])\n",
    "tensor_img = composed(dataloader[0])['RGB']\n",
    "print(type(tensor_img))\n",
    "print(\"Max: {0}, Min: {1}\".format(np.max(tensor_img.numpy()), np.min(tensor_img.numpy())))\n",
    "plt.imshow(tensor_img.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# SHOW A NUMPY IMAGES CONVERTED FROM A TENSOR USING PYTORCH TRANSFORMS\n",
    "composed = transforms.Compose([transforms.ToTensor()])\n",
    "tensor_img = composed(dataloader[0]['RGB'])\n",
    "print(type(tensor_img))\n",
    "print(\"Max: {0}, Min: {1}\".format(np.max(tensor_img.numpy()), np.min(tensor_img.numpy())))\n",
    "plt.imshow(tensor_img.numpy().transpose((1, 2, 0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToTensor(object):\n",
    "    \"\"\"Convert ndarray in sample to Tensors.\"\"\"\n",
    "    \n",
    "    def __call__(self, sample_img):\n",
    "        if len(np.shape(sample_img)) > 2:\n",
    "            # swap colour axis because\n",
    "            # numpy image: H x W x C\n",
    "            # torch image: C x H x W\n",
    "            return torch.from_numpy(sample_img.transpose((2, 0, 1)))\n",
    "            #rgb_img = np.moveaxis(np.asarray(rgb_img), [-1], [0])\n",
    "        else:\n",
    "            return torch.from_numpy(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using class function\n",
    "\n",
    "Transformation can be performed when class instantiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data[0]['DEPTH'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([transforms.ToTensor()])\n",
    "load_data = LoadData(root_dir=data_folder, transform=composed)\n",
    "\n",
    "ax = plt.subplot(2, 1, 1)\n",
    "plt.imshow(load_data[0]['RGB'].numpy().transpose((1, 2, 0)))\n",
    "ax = plt.subplot(2, 1, 2)\n",
    "plt.imshow(load_data[0]['DEPTH'][0].numpy())\n",
    "print(np.max(load_data[0]['DEPTH'][0].numpy()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data = LoadData(root_dir=data_folder)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "class Normalization(object):\n",
    "    \"\"\"Normalization for a given 2d array\"\"\"     \n",
    "    def __call__(self, sample):\n",
    "        rgb_img, depth_img = sample['RGB'], sample['DEPTH']\n",
    "\n",
    "        # RGB IMAGE NORMALIZATION W.R.T DIMENSIONS\n",
    "        rgb_img_norm_ch0 = self._normalizer(rgb_img[:, :, 0])\n",
    "        rgb_img_norm_ch1 = self._normalizer(rgb_img[:, :, 1])\n",
    "        rgb_img_norm_ch2 = self._normalizer(rgb_img[:, :, 2])\n",
    "        \n",
    "        rgb_img = np.stack((rgb_img_norm_ch0, rgb_img_norm_ch1, rgb_img_norm_ch2))\n",
    "        \n",
    "        # swap colour axis because\n",
    "        # numpy image: H x W x C\n",
    "        # torch image: C x H x W\n",
    "        rgb_img = rgb_img.transpose((1, 2, 0))\n",
    "        \n",
    "        # DEPTH IMAGE NORMALIZATION\n",
    "        depth_img = self._normalizer(depth_img)\n",
    "        \n",
    "        return {'RGB': rgb_img, 'DEPTH': depth_img}\n",
    "        \n",
    "    def _normalizer(self, array_2d):\n",
    "        array_scaled = (array_2d - np.min(array_2d)) / (np.max(array_2d) - np.min(array_2d))\n",
    "        #print(np.max(array_scaled), np.min(array_scaled))\n",
    "        \n",
    "        return array_scaled"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "norm = Normalization()\n",
    "norm_img = norm(load_data[0])\n",
    "print('MAX: {0}, MIN: {1}'.format(np.max(norm_img['RGB']), np.min(norm_img['RGB'])))\n",
    "print('MAX: {0}, MIN: {1}'.format(np.max(norm_img['DEPTH']), np.min(norm_img['DEPTH'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalization(object):\n",
    "    \"\"\"Normalization for a given 2d array\"\"\"     \n",
    "    def __call__(self, sample_img):        \n",
    "        if len(np.shape(sample_img)) > 2:\n",
    "            # RGB IMAGE NORMALIZATION W.R.T DIMENSIONS\n",
    "            rgb_img_norm_ch0 = self._normalizer(sample_img[:, :, 0])\n",
    "            rgb_img_norm_ch1 = self._normalizer(sample_img[:, :, 1])\n",
    "            rgb_img_norm_ch2 = self._normalizer(sample_img[:, :, 2])\n",
    "        \n",
    "            rgb_img = np.stack((rgb_img_norm_ch0, rgb_img_norm_ch1, rgb_img_norm_ch2))\n",
    "        \n",
    "            # swap colour axis because\n",
    "            # numpy image: H x W x C\n",
    "            # torch image: C x H x W\n",
    "            return rgb_img.transpose((1, 2, 0))\n",
    "        \n",
    "        else:\n",
    "            # DEPTH IMAGE NORMALIZATION\n",
    "            return self._normalizer(sample_img)\n",
    "        \n",
    "    def _normalizer(self, array_2d):\n",
    "        array_scaled = (array_2d - np.min(array_2d)) / (np.max(array_2d) - np.min(array_2d))\n",
    "        #print(np.max(array_scaled), np.min(array_scaled))\n",
    "        \n",
    "        return array_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.shape(load_data[0]['DEPTH'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAX: 1.0, MIN: 0.0\n"
     ]
    }
   ],
   "source": [
    "norm = Normalization()\n",
    "norm_img = norm(load_data[0]['DEPTH'])\n",
    "print('MAX: {0}, MIN: {1}'.format(np.max(norm_img), np.min(norm_img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "composed = transforms.Compose([Normalization(), ToTensor()])\n",
    "load_data = LoadData(root_dir=data_folder, transform=composed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DEPTH': tensor([[0.7612, 0.7612, 0.7613,  ..., 0.1595, 0.1595, 0.1595],\n",
       "         [0.7612, 0.7612, 0.7613,  ..., 0.1595, 0.1595, 0.1595],\n",
       "         [0.7612, 0.7612, 0.7612,  ..., 0.1595, 0.1595, 0.1595],\n",
       "         ...,\n",
       "         [0.2507, 0.2507, 0.2508,  ..., 0.0534, 0.0534, 0.0535],\n",
       "         [0.2507, 0.2508, 0.2508,  ..., 0.0533, 0.0534, 0.0534],\n",
       "         [0.2508, 0.2508, 0.2508,  ..., 0.0533, 0.0534, 0.0534]]),\n",
       " 'RGB': tensor([[[0.9569, 0.9882, 0.9569,  ..., 0.9529, 1.0000, 1.0000],\n",
       "          [0.9647, 0.9882, 0.9608,  ..., 1.0000, 1.0000, 0.9608],\n",
       "          [0.9882, 0.9882, 0.9843,  ..., 0.9843, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9922, 0.9765, 1.0000,  ..., 0.9765, 0.9961, 0.9765],\n",
       "          [1.0000, 0.9922, 0.9961,  ..., 0.9765, 0.9961, 0.9882],\n",
       "          [1.0000, 0.9843, 0.9961,  ..., 0.9647, 0.9882, 1.0000]],\n",
       " \n",
       "         [[0.9843, 1.0000, 0.9765,  ..., 0.9294, 1.0000, 1.0000],\n",
       "          [0.9843, 1.0000, 0.9804,  ..., 0.9961, 1.0000, 0.9608],\n",
       "          [1.0000, 1.0000, 0.9922,  ..., 0.9686, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [0.9882, 0.9765, 1.0000,  ..., 1.0000, 1.0000, 0.9843],\n",
       "          [1.0000, 0.9922, 1.0000,  ..., 1.0000, 1.0000, 0.9961],\n",
       "          [1.0000, 0.9843, 1.0000,  ..., 0.9922, 1.0000, 1.0000]],\n",
       " \n",
       "         [[0.9569, 0.9843, 0.9529,  ..., 0.9294, 0.9804, 0.9725],\n",
       "          [0.9608, 0.9843, 0.9569,  ..., 0.9922, 0.9804, 0.9294],\n",
       "          [0.9843, 0.9843, 0.9725,  ..., 0.9647, 0.9804, 0.9725],\n",
       "          ...,\n",
       "          [1.0000, 0.9843, 1.0000,  ..., 0.9843, 0.9804, 0.9412],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9843, 0.9725, 0.9529],\n",
       "          [1.0000, 0.9922, 1.0000,  ..., 0.9647, 0.9647, 0.9569]]],\n",
       "        dtype=torch.float64)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
